{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Code Uses Selenium and BeautifulSoup as the main libraries to scrape the data from 99acres. Run all the blocks to get the data from first 30 pages of the website. Page limit can be increased to N. If page Limit will increase then the time taken by the scraper will be more. Time Taken to get the text for 30 pages is 10 Min.\n",
    "\n",
    "To get the data of properties in four Big Metro Cities in India.\n",
    "1. Data Collection is done with the help of Web Scraping using Selenium and Language Used is Python.\n",
    "2. Website We took for Data Collection :- 99acres.com\n",
    "3. We choose 99 acres because one property is only live for 60 days and we get the new Under Construction or          Ready to move properties easily.\n",
    "4. Data we scrape consist of 3100 rows and 12 columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "import re\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_multiple_pages(chrome, city_name, page_sources):\n",
    "    for i in range(1,30):\n",
    "        if city_name == 'delhi':\n",
    "            url = 'https://www.99acres.com/property-in-delhi-ncr-ffid-page-'+str(i)\n",
    "        elif city_name == 'mumbai':\n",
    "            url = 'https://www.99acres.com/property-in-mumbai-ffid-page-'+str(i)\n",
    "        elif city_name == 'chennai':\n",
    "            url = 'https://www.99acres.com/property-in-chennai-ffid-page-'+str(i)\n",
    "        elif city_name == 'bangalore':\n",
    "            url = 'https://www.99acres.com/property-in-bangalore-ffid-page-'+str(i)\n",
    "        time.sleep(5)\n",
    "        chrome.execute_script(\"window.open('');\")\n",
    "        # Switch to the new window and open URL B\n",
    "        chrome.switch_to.window(chrome.window_handles[1])\n",
    "        chrome.get(url)\n",
    "        time.sleep(10)\n",
    "        new_data = chrome.page_source\n",
    "        page_sources.append(new_data)\n",
    "        chrome.close()\n",
    "        chrome.switch_to.window(chrome.window_handles[0])\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBhk(container):\n",
    "    try:\n",
    "        bhk = container.find('td', id='srp_tuple_bedroom').text\n",
    "    except AttributeError:\n",
    "        bhk = None\n",
    "        pass\n",
    "    if bhk is not None:\n",
    "        try:\n",
    "            bhk = re.search('(.+)BHK', bhk).group(1)\n",
    "        except AttributeError:\n",
    "            bhk = re.search('(.+)RK', bhk).group(1)\n",
    "    else:\n",
    "        pass\n",
    "    return bhk\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBathroom(container):\n",
    "    try:\n",
    "        bathroom = container.find('div', id='srp_tuple_bathroom').text\n",
    "    except AttributeError:\n",
    "        bathroom = None\n",
    "    if bathroom is not None:\n",
    "        bathroom = re.search('(.+)Bath', bathroom).group(1)\n",
    "    else:\n",
    "        pass\n",
    "    return bathroom\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions to get the text out from the Html content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(house_containers, city_name, address, name, price, price_per_sqft, total_sqft, square_mt_area, bhk, bathroom, availability):\n",
    "    for container in house_containers:\n",
    "        addres = container.find_all('h2')[0].text\n",
    "        print('Address', addres)\n",
    "        namee = container.find_all('td', class_='list_header_bold srpTuple__spacer10')[0].text\n",
    "        print('Name', namee)\n",
    "        pricee = container.find('td', id='srp_tuple_price').text\n",
    "        print('Price', pricee)\n",
    "        try:\n",
    "            pricee = re.search('(.+)â‚¹', pricee).group(1)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        try:\n",
    "            price_per_sqft_ = container.find('div', id='srp_tuple_price_per_unit_area').text\n",
    "        except AttributeError:\n",
    "            price_per_sqft_ = 0 \n",
    "        print('price_per_Sqft', price_per_sqft_)\n",
    "        total_sqft_ = container.find('td', id='srp_tuple_primary_area').text\n",
    "        total_sqft_ = re.search('(.+)sq.ft.', total_sqft_).group(1)\n",
    "        square_mt_area_ = container.find('div', id='srp_tuple_secondary_area').text\n",
    "        square_mt_area_ = re.search('(.+)sq.m.', square_mt_area_).group(1)\n",
    "        square_mt_area_ = square_mt_area_.lstrip('(') \n",
    "        bhk_ = getBhk(container)\n",
    "        bathroom_ = getBathroom(container)\n",
    "        availability_ = container.find('div', id='srp_tuple_secondary_tags_0').text\n",
    "        address.append(addres)\n",
    "        name.append(namee)\n",
    "        price.append(pricee)\n",
    "        price_per_sqft.append(price_per_sqft_)\n",
    "        total_sqft.append(total_sqft_)\n",
    "        square_mt_area.append(square_mt_area_)\n",
    "        bhk.append(bhk_)\n",
    "        bathroom.append(bathroom_)\n",
    "        availability.append(availability_)\n",
    "    return \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lists(page_sources):\n",
    "    address = []\n",
    "    name = []\n",
    "    price = []\n",
    "    price_per_sqft = []\n",
    "    total_sqft = []\n",
    "    square_mt_area = []\n",
    "    bhk = []\n",
    "    bathroom = []\n",
    "    availability = []\n",
    "    for i in page_sources:\n",
    "        html_soup = BeautifulSoup(data, 'html.parser')\n",
    "        house_containers = html_soup.find_all('table', class_=\"srpTuple__tupleTable\")\n",
    "        empty_return = getData(house_containers, city_name, address, name, price, price_per_sqft, total_sqft, square_mt_area, bhk, bathroom, availability)\n",
    "    return address, name, price, price_per_sqft, total_sqft, square_mt_area, bhk, bathroom, availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(address, name, price, price_per_sqft, total_sqft, square_mt_area, bhk, bathroom, availability, city_name):\n",
    "    data = {'Location': address, 'Name_of_The_property': name,'Bhk': bhk, 'Availability': availability, 'Totat_sqft': total_sqft,\n",
    "        'Price_per_sqft': price_per_sqft, 'Total_price': price, 'Bathroom': bathroom, 'Square_mt_area': square_mt_area, 'City': city_name}\n",
    "    df = pd.DataFrame(data)\n",
    "    name = city_name+'_data.csv'\n",
    "    df.to_csv(name)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main function to be run to get the Scrapper data from 99acres website. The function will ask you to enter the city from which you want to scape data. The script can be used for any city which is listed on 99acres. I have used Selenium and Beautiful soup to get the data from the 99acres and saved that data as a csv file which is starts with the name of city followed by 'data.csv'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chrome = webdriver.Chrome(executable_path=r'/home/katik/Downloads/chromedriver')\n",
    "    chrome.get(\"https://www.99acres.com/\")\n",
    "    print(chrome.title)\n",
    "    textField = \"keyword\"\n",
    "    wait = WebDriverWait(chrome, 20)\n",
    "    properties = wait.until(lambda driver: driver.find_element_by_xpath(\n",
    "    '//*[@id=\"keyword\"]'))\n",
    "    city_name = input(\"Enter the city for which you want to search :-\")\n",
    "#     city_name = 'delhi'\n",
    "    properties.send_keys(city_name)\n",
    "    wait.until(ec.visibility_of_element_located((By.XPATH, '//*[@id=\"submit_query\"]')))\n",
    "    chrome.find_element_by_xpath('//*[@id=\"submit_query\"]').click()\n",
    "    time.sleep(10)\n",
    "    page_sources = []\n",
    "    data = chrome.page_source\n",
    "    page_sources.append(data)\n",
    "    get_data_multiple_pages(chrome, city_name, page_sources)\n",
    "    chrome.close()\n",
    "    address, name, price, price_per_sqft, total_sqft, square_mt_area, bhk, bathroom, availability = get_lists(page_sources)\n",
    "    get_dataframe(address, name, price, price_per_sqft, total_sqft, square_mt_area, bhk, bathroom, availability, city_name)\n",
    "    chrome.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_bangalore = pd.read_csv('bangalore_data.csv')\n",
    "dataframe_delhi = pd.read_csv('delhi_data.csv')\n",
    "dataframe_mumbai = pd.read_csv('mumbai_data.csv')\n",
    "dataframe_chennai = pd.read_csv('chennai_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.concat([dataframe_bangalore, dataframe_chennai, dataframe_delhi, dataframe_mumbai], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('MetroCitiesDataNew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
